{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.io.json import json_normalize\n",
    "import spacy as sp\n",
    "import en_core_web_sm\n",
    "import string\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from xgboost.sklearn import XGBClassifier \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdata_json = pd.read_json('../relevantNewsNLTK4.json')\n",
    "irdata_json = pd.read_json('../irrelevantNewsNLTK4.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdata_json['relevance']=1\n",
    "irdata_json['relevance']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>headline</th>\n",
       "      <th>source</th>\n",
       "      <th>summary</th>\n",
       "      <th>uid</th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>lowly milkshake weapon choice britons determin...</td>\n",
       "      <td>Milkshakes become weapon of choice in UK Europ...</td>\n",
       "      <td>Agence France Presse</td>\n",
       "      <td>Former UK Independence Party leader Nigel Fara...</td>\n",
       "      <td>a437ff48-104a-54bb-bff7-c7a736158524</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>anz race set bring biggest white collar job lo...</td>\n",
       "      <td>ANZ's first assault in the looming job armageddon</td>\n",
       "      <td>News Ltd.</td>\n",
       "      <td>ANZ has moved to the front of the race that’ s...</td>\n",
       "      <td>366c92af-8143-5ffa-8702-4f26bd22c8b6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>jul 10 carnival cruise stateroom attendants ex...</td>\n",
       "      <td>Carnival Cruise Line to collect your used soap...</td>\n",
       "      <td>Tribune Content Agency</td>\n",
       "      <td>Jul. 10-- Carnival Cruise Line stateroom atten...</td>\n",
       "      <td>863096d4-48f0-5a7c-bee6-384a76d575ee</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>chennai rohit 543 saravanan 546 wickets standa...</td>\n",
       "      <td>Standard CC bags fourth title in a row [New In...</td>\n",
       "      <td>SyndiGate Media Inc.</td>\n",
       "      <td>CHENNAI: R Rohit and P Saravanan took five wic...</td>\n",
       "      <td>3e4d6490-4224-595e-be26-4cb249209b8f</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>donald trump nominee lead fish wildlife servic...</td>\n",
       "      <td>Revealed: Trump's Wildlife Service pick has ti...</td>\n",
       "      <td>Guardian</td>\n",
       "      <td>New revelations show she also has ties to the ...</td>\n",
       "      <td>9f3e248d-b040-5058-bdc9-61c4de59f02a</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  lowly milkshake weapon choice britons determin...   \n",
       "1  anz race set bring biggest white collar job lo...   \n",
       "2  jul 10 carnival cruise stateroom attendants ex...   \n",
       "3  chennai rohit 543 saravanan 546 wickets standa...   \n",
       "4  donald trump nominee lead fish wildlife servic...   \n",
       "\n",
       "                                            headline                  source  \\\n",
       "0  Milkshakes become weapon of choice in UK Europ...    Agence France Presse   \n",
       "1  ANZ's first assault in the looming job armageddon               News Ltd.   \n",
       "2  Carnival Cruise Line to collect your used soap...  Tribune Content Agency   \n",
       "3  Standard CC bags fourth title in a row [New In...    SyndiGate Media Inc.   \n",
       "4  Revealed: Trump's Wildlife Service pick has ti...                Guardian   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Former UK Independence Party leader Nigel Fara...   \n",
       "1  ANZ has moved to the front of the race that’ s...   \n",
       "2  Jul. 10-- Carnival Cruise Line stateroom atten...   \n",
       "3  CHENNAI: R Rohit and P Saravanan took five wic...   \n",
       "4  New revelations show she also has ties to the ...   \n",
       "\n",
       "                                    uid  index  label  relevance  \n",
       "0  a437ff48-104a-54bb-bff7-c7a736158524      0      1          1  \n",
       "1  366c92af-8143-5ffa-8702-4f26bd22c8b6      1      1          1  \n",
       "2  863096d4-48f0-5a7c-bee6-384a76d575ee      2      1          1  \n",
       "3  3e4d6490-4224-595e-be26-4cb249209b8f      3      1          1  \n",
       "4  9f3e248d-b040-5058-bdc9-61c4de59f02a      4      1          1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = rdata_json.append(irdata_json, ignore_index=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = en_core_web_sm.load()\n",
    "stop_words = sp.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer using spaCy\n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    # Removing spaces and converting text into lowercase\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['content'] # the features we want to analyze\n",
    "ylabels = data['relevance'] # the labels, or answers, we want to test against\n",
    "\n",
    "# X_train and y_train are the entire dataset (for now)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Use this for MultinomialNB (Naive Bayes) Classifier in the pipeline after vectorizer\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cleaner', <__main__.predictors object at 0x000002100F8402E8>),\n",
       "                ('vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=No...\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "                               max_delta_step=0, max_depth=3,\n",
       "                               min_child_weight=1, missing=None,\n",
       "                               n_estimators=100, n_jobs=1, nthread=None,\n",
       "                               objective='binary:logistic', random_state=0,\n",
       "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "                               seed=None, silent=None, subsample=1,\n",
       "                               verbosity=1))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classifier\n",
    "# Note: classifier is a placeholder for now\n",
    "\n",
    "classifier =XGBClassifier()\n",
    "\n",
    "#('to_dense', DenseTransformer()),\n",
    "# Create pipeline using Bag of Words\n",
    "pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# model generation\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73775\n",
      "Precision: 0.7411944869831547\n",
      "Recall: 0.728184553660983\n"
     ]
    }
   ],
   "source": [
    "# Predicting with a test dataset\n",
    "# Note: this will fail if testing data size is 0\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(classifier, open('best_bow_xgboost_model.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
