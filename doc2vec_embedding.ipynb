{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import seaborn as sns\n",
    "from xgboost.sklearn import XGBClassifier \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('relevant_news_10K.json',encoding=\"utf8\") as f1:\n",
    "    data1 = json.load(f1)\n",
    "    \n",
    "with open('irrelevant_news_10K.json', encoding = \"utf8\") as f2:\n",
    "    data2 = json.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = []\n",
    "summaries = []\n",
    "sources = []\n",
    "content = []\n",
    "\n",
    "for d in data1:\n",
    "    headlines.append(d['headline'])\n",
    "    summaries.append(d['summary'])\n",
    "    sources.append(d['source'])\n",
    "    content.append(d['content'])\n",
    "    \n",
    "for d in data2:\n",
    "    headlines.append(d['headline'])\n",
    "    summaries.append(d['summary'])\n",
    "    sources.append(d['source'])\n",
    "    content.append(d['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [[1]]*len(data1)\n",
    "label.extend([[0]]*len(data2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use content as our data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(content, label, test_size=0.20, random_state=42, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(corpus, tokens_only=False):\n",
    "    #with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "    for i, line in tqdm_notebook(enumerate(corpus), total = len(corpus)):\n",
    "        if tokens_only:\n",
    "            yield gensim.utils.simple_preprocess(line)\n",
    "        else:\n",
    "            # For training data, add tags\n",
    "            yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c47dc4d4f946ce9186564b2449c4c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "825aef79360c4d46b5f2018e83c3e46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_corpus = list(read_corpus(X_train))\n",
    "test_corpus = list(read_corpus(X_test, tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune parameters <TODO>\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=2, epochs=10, workers= 4,compute_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different vocab builds <TODO>\n",
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLogger(CallbackAny2Vec):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "             print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "            print(\"Epoch #{} end\".format(self.epoch))\n",
    "            #print(\"Loss #{}\".format(model.get_latest_training_loss()))\n",
    "            self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "Epoch #0 start\n",
      "Epoch #0 end\n",
      "Epoch #1 start\n",
      "Epoch #1 end\n",
      "Epoch #2 start\n",
      "Epoch #2 end\n",
      "Epoch #3 start\n",
      "Epoch #3 end\n",
      "Epoch #4 start\n",
      "Epoch #4 end\n",
      "Epoch #5 start\n",
      "Epoch #5 end\n",
      "Epoch #6 start\n",
      "Epoch #6 end\n",
      "Epoch #7 start\n",
      "Epoch #7 end\n",
      "Epoch #8 start\n",
      "Epoch #8 end\n",
      "Epoch #9 start\n",
      "Epoch #9 end\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "epoch_logger = EpochLogger()\n",
    "model.train(train_corpus,  total_examples=model.corpus_count,epochs=model.epochs, callbacks=[epoch_logger])\n",
    "#model.get_latest_training_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2094"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vocab['oil'].count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf053f5640b4c95bbf927876359ffc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in tqdm_notebook(range(len(train_corpus))):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "    \n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 14963,\n",
       "         1: 882,\n",
       "         7: 6,\n",
       "         6: 10,\n",
       "         4: 22,\n",
       "         48: 1,\n",
       "         14: 3,\n",
       "         2: 41,\n",
       "         15: 3,\n",
       "         3: 20,\n",
       "         5: 13,\n",
       "         18: 2,\n",
       "         13: 5,\n",
       "         11: 2,\n",
       "         31: 1,\n",
       "         12: 4,\n",
       "         10: 2,\n",
       "         8: 5,\n",
       "         34: 1,\n",
       "         23: 2,\n",
       "         9: 1,\n",
       "         17: 3,\n",
       "         20: 1,\n",
       "         627: 1,\n",
       "         786: 1,\n",
       "         16: 1,\n",
       "         71: 1,\n",
       "         168: 1,\n",
       "         19: 1,\n",
       "         27: 1})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ranks)   #93% of the time found to be similar to the same document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (2238): «may corelogic will close its bloomington office and up to employees will be laid off the irvine calif based provider of real estate property information analytics and data services filed notice with the minnesota department of employment and economic development that its office at th ave will close by dec layoffs will begin july the notice was required under the worker adjustment and retraining notification act the bloomington office is part of the company appraisal operation some of the work will be transitioned to location in dallas and workers may apply for jobs there said company spokeswoman alyson austin via mail corelogic said in december that it would be accelerating its multiyear restructuring plan that includes the appraisal management operations the publicly traded company had total employees as of dec of them in the united states according to its most recent annual report with the securities and exchange commission corelogic has properties in states and nine foreign countries corelogic does not have any other offices in minnesota the company has more than billion records going back years on properties mortgages and other real estate information and mines that data to provide advice and services to its clients the company annual revenue has declined each of the previous two years it had revenue of billion in down from the previous year net income for was million down the star tribune minneapolis visit the star tribune minneapolis at www startribune com distributed by tribune content agency llc»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d100,n5,w5,mc2,s0.001,t4):\n",
      "\n",
      "MOST (11152, 0.9497233033180237): «new york june prnewswire nielsen announced today it has expanded advertising measurement on youtube mobile app with nielsen digital ad ratings to additional markets including belgium brazil bulgaria czech republic greece hong kong hungary india indonesia ireland israel malaysia mexico netherlands new zealand norway philippines poland puerto rico singapore south africa spain taiwan thailand turkey and the united arab emirates now available in global markets this service builds on nielsen current measurement coverage of youtube ads on desktop and mobile web browsers to provide marketers independent and comprehensive cross device measurement of advertising audiences on youtube across computer and mobile devices measurement of digital ad ratings for youtube mobile app was first made available in the united states in with this global expansion media buyers and sellers will have access to age and gender demographics for consumers viewing advertising on the youtube mobile app as well as reach frequency and gross rating points grps measurement of youtube ads uses consistent methodology with all other mobile publishers in digital ad ratings enabling both media buyers and sellers to leverage truly comparable and deduplicated persons based measurement across publishers in their media planning and execution with this measurement publishers advertisers and media agencies will be able to gain deeper understanding of their audience across youtube using metrics comparable to those used for tv providing marketers with independent measurement of youtube mobile audiences globally is major step forward for the industry said amanda tarpey svp of digital product leadership with the expansion of digital ad ratings to measure audiences inclusive of youtube sizeable mobile footprint advertisers in more than markets will have the comparability coverage and transparency they need to maximize the impact of their digital media around the world nielsen holdings plc nlsn is global measurement and data analytics company that provides the most complete and trusted view available of consumers and markets worldwide our approach marries proprietary nielsen data with other data sources to help clients around the world understand what happening now what happening next and how to best act on this knowledge for more than years nielsen has provided data and analytics based on scientific rigor and innovation continually developing new ways to answer the most important questions facing the media advertising retail and fast moving consumer goods industries an company nielsen has operations in over countries covering more than of the world population for more information visit www nielsen com view original content to download multimedia http www prnewswire com news releases nielsen launches global measurement for youtube mobile app html»\n",
      "\n",
      "MEDIAN (469, 0.2470003068447113): «house oversight committee chairman elijah cummings sent letter wednesday asking homeland security to reveal when officials learned about secret facebook groups frequented by border patrol agents where offensive messages were exchanged customs and border protection the agency that oversees the border patrol is investigating dozens of agents and some have already been relieved of law enforcement duties officials told congress last week but mr cummings said he worried other offenders are still working with immigrants and children despite their involvement with the facebook group he asked for names and duties assigned for every cbp employee who is being investigated and what has happened to them during the probe mr cummings said he also needs an immediate briefing on the matter the committee is investigating racist sexist and xenophobic comments relating to immigrants and members of congress made by employees of customs and border protection cbp in secret facebook groups the chairman said the facebook groups were first revealed in early july by propublica among messages it found were ones mocking migrants who died attempting to jump the border and ones belittling members of congress such as rep alexandria ocasio cortez who was depicted in messages containing photos manipulated to appear as if she was being forced to perform sex acts the homeland security inspector general is already investigating to see if supervisors were aware of the facebook groups and whether they failed to take action when necessary cbp own office of professional responsibility is probing individual agents behavior but border officials rejected claims that they foster subculture of intolerance or abuse the vast vast majority point whatever percent of our men and women are good hard working american citizens who are doing the best they can in very very difficult crisis border patrol chief carla provost told the house last week»\n",
      "\n",
      "LEAST (10456, -0.18646140396595): «may may for long time on tuesday morning jim lake jr just stood there in the middle of belleview street staring at the horror staring because he could not speak and when he finally could after he collected himself after he choked back the emotion he said this of the gut wrenching devastation that lie only few feet in front of him smoldering in the early morning sunlight am in shock am so disheartened he said am just so sorry for the city just pause in front of us fire trucks with ladders blasted water at the smoldering ruins of lake building the ambassador hotel which opened in as the majestic hotel once billed as the most elegantly appointed hotel in the south at times tuesday the hotel seemed to disappear behind shroud of smoke felt like was entrusted to restore it lake began again we were getting there in fits and starts and then then this happens amanda moreno lake wife stood next to him wiping away tears as though loved one had just died moreno and lake could have turned the ambassador into hotel long ago some lodging chains wanted in but lake and moreno waved off all comers insisting they would do the work themselves because we rather have something dallas residents could be part of and call home lake told me in february lake responsible for the rebirths of the bishop arts district jefferson tower and mayor george sergeant year old home on north zang boulevard had given me tour of the ambassador only three months ago it was my first and now only time inside the long vacant building that historic designation documents refer to as the oldest luxury hotel in dallas its strange history included visits from theodore roosevelt sarah bernhardt woodrow wilson joey ramone william howard taft and morrissey lake had bought the hotel in he began the restoration work two years later converting the hotel into month micro apartments with pool and restaurant and basement speakeasy and ground floor retail when visited crews had begun to strip the insides of its toxic leftovers yet even in decay and disrepair it felt singular faded yesterday worthy of countless more tomorrows but aside from some remediation the redo had not yet begun in earnest because lake needed the city council to approve closing off st paul street at east griffin street to make way for the ambassador new driveway the dallas city council approved that closure last wednesday council members did not discuss or debate the agenda item it took only seconds of their time the years it took to get to that minute at city hall were now all for nothing dallas fire rescue on tuesday began their own plans to put out any remaining hot spots then prepare for crews to level what remains of the ambassador laying the building down fire engineer called it tuesday morning as he shuffled the names of firefighters coming on duty to spell colleagues who began wrestling the inferno shortly after there will be no salvage said the firefighter of the history that died tuesday morning investigators will get in there too to determine whether the cause was manslaughter or suicide hard as it is to believe this devastation could have been worse standing off to the side in the shadow of the year old hughes brothers candy factory building on ervay jerry nappi iii the building caretaker told his co workers how he narrowly escaped dying in the fire his trailer was parked on the st paul side and at tuesday as he lay sleeping nappi said homeless man began yelling for him to wake up get out he rubbed the sleep from his eyes and went outside he could see nothing there was too much smoke nappi had only time enough to collect his dog get into his truck and drive could hear windows shattering coming down he said within minutes side of the wall came down smashing his trailer his home lost everything own nappi said except what am wearing black ball cap black shirt blue jeans shoes know minutes may sound like lot but it wasn lake said he found out about the fire from his son scott who was coming home from fort worth early tuesday morning at first scott thought the fire was at fair park but as he got closer he realized the ambassador was ablaze at he texted his father with photo of flames shooting out the windows on the ervay side jim lake showed me that photo his hands shook as he held the phone it was scott who first got jim interested in the ambassador it was scott who brokered the deal even in the fog of grief jim lake couldn help himself he pointed toward the building and the red clay brick peeking out from behind the white exterior see that arch that must have been the original entrance before they covered it up in the the fire revealed history even while consuming it little later in the morning jim lake walked over to the mountain of bricks spilled along ervay for moment he started at the building what you could see of it anyway behind the shroud of smoke and water he then leaned over and grabbed brick which he passed around to colleagues standing watch by fire command lake was awed by its weight heavier than any brick ve ever held he said solid he said he would try to salvage as much of the ruins as possible and use them to construct whatever comes next it will still be the ambassador lake said knowing full well it won be only pieces of it throughout the morning onlookers began filing out to stare at the charred remains tv news crews remained at distance firefighters took photos once they were told of the hotel age its significance the special place it holds for so many who will never see this part of downtown the same away again initially dallas fire rescue planned to go ahead and take down what remains of the building because they have deemed it unsafe but the city has no equipment capable of razing the ambassador so lake spent the morning with contractors discussing bids planning the funeral instead of the resurrection just wish there was way we could save it lake said over and over just wish there was way to save it but there is not and now another piece of dallas history is gone erased by fire disaster damned tragedy the dallas morning news visit the dallas morning news at www dallasnews com distributed by tribune content agency llc»\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus))\n",
    "inferred_vector = model.infer_vector(test_corpus[1876])  #doc_id\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model.save('doc2vec_embedding_1.model')   #memory error !! <TODO> optimization \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature vectors from Doc2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doc2vec feature vectors\n",
    "X_train = np.array([model.docvecs[i] for i in range(len(train_corpus))])\n",
    "X_test = np.array([model.infer_vector(test_corpus[i]) for i in range(len(test_corpus))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "xclas = XGBClassifier()  # and for classifier  \n",
    "xclas.fit(X_train, np.array(Y_train))\n",
    "y_pred = xclas.predict(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.60      0.64      1981\n",
      "           1       0.65      0.73      0.69      2019\n",
      "\n",
      "    accuracy                           0.67      4000\n",
      "   macro avg       0.67      0.67      0.67      4000\n",
      "weighted avg       0.67      0.67      0.67      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1198  783]\n",
      " [ 540 1479]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66925\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.721375\n"
     ]
    }
   ],
   "source": [
    "#Sanity Check: test on train data\n",
    "#Expected Result: Should overfit\n",
    "xclas.fit(X_train, np.array(Y_train))\n",
    "y_pred_1 = xclas.predict(X_train)\n",
    "print(accuracy_score(np.array(Y_train),y_pred_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data: 67% Train Data: 72% -> Model has not learnt enough!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61175\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred_gnb = gnb.fit(X_train, np.array(Y_train)).predict(X_test)\n",
    "print(accuracy_score(np.array(Y_test),y_pred_gnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6005625\n"
     ]
    }
   ],
   "source": [
    "y_pred_gnb_1 = gnb.fit(X_train, np.array(Y_train)).predict(X_train)\n",
    "print(accuracy_score(np.array(Y_train),y_pred_gnb_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data: 60% Train Data: 61%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65825\n"
     ]
    }
   ],
   "source": [
    "logistic = LogisticRegression()\n",
    "y_pred_lgr = logistic.fit(X_train, np.array(Y_train)).predict(X_test)\n",
    "print(accuracy_score(np.array(Y_test),y_pred_lgr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6656875\n"
     ]
    }
   ],
   "source": [
    "y_pred_lgr_1 = logistic.fit(X_train, np.array(Y_train)).predict(X_train)\n",
    "print(accuracy_score(np.array(Y_train),y_pred_lgr_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data: 65% Train Data: 66%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODOS  \n",
    "* Pretty low accuracy, features not representative \n",
    "* need to fine-tune embedding model and re-train with different parameters\n",
    "* Experiment with different embedding models\n",
    "* Performance of Word2vec > Doc2vec, Try mean(word2vec) of each document\n",
    "* Unable to save embedding_model, reduce feature vector_size?\n",
    "* Try other classifiers and other feature sets (tf-idf, bow)\n",
    "*  Current model can be used for explainability model training experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
